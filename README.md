# ğŸ§  AI Bootcamp Project â€” Image Classification 

### Team G3 â€” Mariana â€¢ Cristina â€¢ Adrian â€¢ Kira  

---

## ğŸ“˜ Executive Summary

This project explores **image classification** using the **CIFAR-10 dataset**, containing 60,000 color images (32Ã—32 pixels) divided into 10 categories (e.g., airplane, car, cat, dog).  

After testing 19 model variations, the **winning model** used **Transfer Learning with VGG16** as a base model and achieved an **accuracy of 86%** on the test set.

---
## ğŸ“‚ Project files


| File / Folder | Description |
|----------------|--------------|
| ğŸ“‚ **history** | Contains unpolished files with different models we tried |
| **Group 3 week 3 presentation.pdf** | Final presentation summarizing project results |
| **README.md** | Project overview and documentation (this file) |
| **Team internal results.xlsx** | Unpolished file used internally to track our learnings |
| **main BEST_VGG16.ipynb** | Final notebook with our winning **Trainer Learning** model ğŸ‘ˆ *start here* |
| **main inhouse CNN.ipynb** | Final notebook with our in-house **CNN model** |

---

## ğŸš€ Project Overview

| Model | Approach | Accuracy | Notes |
|--------|-----------|-----------|-------|
| **Baseline CNN** | Custom convolutional model | ~69% | Initial version |
| **Optimized CNN** | Added batch normalization, dropout, early stopping | ~75â€“80% | Reduced overfitting |
| **Transfer Learning (VGG16)** | Fine-tuned pretrained model | **86%** | Best overall performance |

**Trade-off:** Transfer learning required more preprocessing and tuning, but significantly improved performance and training stability.

---

## ğŸ§© Dataset & Preprocessing

**Dataset:** [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)  
- 60,000 images (50,000 train / 10,000 test)  
- 10 classes, RGB format  

**Steps:**
1. Normalized images to `[0, 1]` range.  
2. Converted labels into categorical (one-hot encoded).  
3. Split data into training, validation, and test sets (80/10/10).  
4. Avoided excessive data augmentation (caused distortions on small images).  

---

## ğŸ¤– Models & Learning Process

### 1ï¸âƒ£ Baseline CNN
- **Layers:** Convolution â†’ MaxPooling â†’ Dense  
- **Results:** Validation accuracy â‰ˆ 69%  
- Served as the foundation for later experiments.

### 2ï¸âƒ£ Improved CNN
- **Changes tested:**
  - Added **batch normalization** â†’ improved convergence  
  - Added **dropout layers** â†’ reduced overfitting  
  - Increased epochs (15 â†’ 100) with **early stopping**  
- **Results:** More stable validation accuracy across datasets.

### 3ï¸âƒ£ Transfer Learning (Winning Model)
- **Base model:** `VGG16` (pretrained on ImageNet)  
- **Additional layers:** Global Average Pooling, Batch Normalization, Dropout  
- **Training setup:**  
  - Fine-tuned top layers only  
  - Used Adam optimizer and early stopping  
- **Results:**  
  - Validation loss: 0.34  
  - Validation accuracy: **86.15%**

---

## ğŸ“Š Evaluation & Learnings

**What worked:**
- Batch normalization and dropout significantly improved performance.  
- Early stopping helped avoid overfitting.  
- Transfer learning drastically improved accuracy and reduced training time.  

**What didnâ€™t:**
- Data augmentation decreased accuracy due to image distortion.  
- Not all pretrained models (MobileNetV2, EfficientNetB0) performed equally well.  

**Key Learnings:**
- Iterative experimentationâ€”changing one parameter at a timeâ€”was essential.  
- Proper model selection (VGG16) can yield substantial gains even with small datasets.  

---

## ğŸ› ï¸ Tech Stack

- **Language:** Python  
- **Libraries:**  
  - TensorFlow / Keras  
  - NumPy  
  - Pandas  
  - Matplotlib / Seaborn  

---

## ğŸ‘©â€ğŸ’» Team

**Mariana Borssatto**  
**Cristina Insignares**  
**AdriÃ¡n A. H.**  
**Kira Redberg**
